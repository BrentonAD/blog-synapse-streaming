{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Structured Streaming in Synapse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:01:07.38995Z",
              "execution_start_time": "2021-10-12T03:01:07.2255088Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5548391Z",
              "session_id": 14,
              "session_start_time": "2021-10-12T03:00:22.5910081Z",
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "storage_account = '<your-storage-account-name>'\n",
        "event_hub = '<your-iot-hub-name>'\n",
        "key_vault = '<your-key-vault-name>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:01:12.7658548Z",
              "execution_start_time": "2021-10-12T03:01:07.4907802Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5566398Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Setup access to storage account for temp data when pushing to Synapse\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", mssparkutils.credentials.getSecret(key_vault,\"adls-key\"))\n",
        "\n",
        "# Setup storage locations for all data\n",
        "ROOT_PATH = f\"abfss://iot@{storage_account}.dfs.core.windows.net/\"\n",
        "BRONZE_PATH = ROOT_PATH + \"bronze/\"\n",
        "SILVER_PATH = ROOT_PATH + \"silver/\"\n",
        "GOLD_PATH = ROOT_PATH + \"gold/\"\n",
        "SYNAPSE_PATH = ROOT_PATH + \"synapse/\"\n",
        "CHECKPOINT_PATH = ROOT_PATH + \"checkpoints/\"\n",
        "\n",
        "# Other initializations\n",
        "IOT_CS = mssparkutils.credentials.getSecret(key_vault,'hub-cs') # IoT Hub connection string (Event Hub Compatible)\n",
        "ehConf = { \n",
        "  'eventhubs.connectionString':sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(IOT_CS),\n",
        "  'ehName':event_hub\n",
        "}\n",
        "\n",
        "# Enable auto compaction and optimized writes in Delta\n",
        "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\",\"true\")\n",
        "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\",\"true\")\n",
        "\n",
        "# Pyspark and ML Imports\n",
        "import os, json, requests\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:02:00.4858568Z",
              "execution_start_time": "2021-10-12T03:02:00.4856499Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5590411Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 14, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "-- Clean up tables & views\n",
        "DROP TABLE IF EXISTS turbine_raw;\n",
        "DROP TABLE IF EXISTS weather_raw;\n",
        "DROP TABLE IF EXISTS turbine_agg;\n",
        "DROP TABLE IF EXISTS weather_agg;\n",
        "DROP TABLE IF EXISTS turbine_enriched;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:02:00.7382561Z",
              "execution_start_time": "2021-10-12T03:02:00.5788951Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5610992Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Make sure root path is empty\n",
        "mssparkutils.fs.rm(ROOT_PATH, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set up streams from IoT Hub "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:02:17.5773429Z",
              "execution_start_time": "2021-10-12T03:02:00.8569594Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5629816Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Schema of incoming data from IoT hub\n",
        "schema = \"timestamp timestamp, deviceId string, temperature double, humidity double, windspeed double, winddirection string, rpm double, angle double\"\n",
        "\n",
        "# Read directly from IoT Hub using the EventHubs library for Spark\n",
        "iot_stream = (\n",
        "\t# Read from IoT Hubs directly\n",
        "\tspark.readStream.format(\"eventhubs\")                                               \n",
        "\t# Use the Event-Hub-enabled connect string\n",
        "    .options(**ehConf)                                                               \n",
        "\t# Load the data\n",
        "    .load()\n",
        "    # Extract the \"body\" payload from the messages\n",
        "\t.withColumn('reading', F.from_json(F.col('body').cast('string'), schema))        \n",
        "\t# Create a \"date\" field for partitioning\n",
        "    .select('reading.*', F.to_date('reading.timestamp').alias('date'))               \n",
        ")\n",
        "\n",
        "# Split our IoT Hub stream into separate streams and write them both into their own Delta locations\n",
        "write_turbine_to_delta = (\n",
        "  iot_stream.filter('temperature is null')                                           # Filter out turbine telemetry from other data streams\n",
        "    .select('date','timestamp','deviceId','rpm','angle')                             # Extract the fields of interest\n",
        "    .writeStream.format('delta')                                                     # Write our stream to the Delta format\n",
        "    .partitionBy('date')                                                             # Partition our data by Date for performance\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_raw\")                   # Checkpoint so we can restart streams gracefully\n",
        "    .start(BRONZE_PATH + \"turbine_raw\")                                              # Stream the data into an ADLS Path\n",
        ")\n",
        "\n",
        "write_weather_to_delta = (\n",
        "  iot_stream.filter(iot_stream.temperature.isNotNull())                              # Filter out weather telemetry only\n",
        "    .select('date','deviceid','timestamp','temperature','humidity','windspeed','winddirection') \n",
        "    .writeStream.format('delta')                                                     # Write our stream to the Delta format\n",
        "    .partitionBy('date')                                                             # Partition our data by Date for performance\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"weather_raw\")                   # Checkpoint so we can restart streams gracefully\n",
        "    .start(BRONZE_PATH + \"weather_raw\")                                              # Stream the data into an ADLS Path\n",
        ")\n",
        "\n",
        "# Create the external tables once data starts to stream in\n",
        "while True:\n",
        "  try:\n",
        "    spark.sql(f'CREATE TABLE IF NOT EXISTS turbine_raw USING DELTA LOCATION \"{BRONZE_PATH + \"turbine_raw\"}\"')\n",
        "    spark.sql(f'CREATE TABLE IF NOT EXISTS weather_raw USING DELTA LOCATION \"{BRONZE_PATH + \"weather_raw\"}\"')\n",
        "    break\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:02:20.455416Z",
              "execution_start_time": "2021-10-12T03:02:17.6890948Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.5652193Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 10
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 10, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [
                  {
                    "metadata": {},
                    "name": "date",
                    "nullable": true,
                    "type": "date"
                  },
                  {
                    "metadata": {},
                    "name": "timestamp",
                    "nullable": true,
                    "type": "timestamp"
                  },
                  {
                    "metadata": {},
                    "name": "deviceId",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "rpm",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "angle",
                    "nullable": true,
                    "type": "double"
                  }
                ],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 5 fields>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql \n",
        "-- We can query the data directly from storage immediately as soon as it starts streams into Delta \n",
        "SELECT * FROM turbine_raw LIMIT 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Silver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:03:49.3359564Z",
              "execution_start_time": "2021-10-12T03:02:20.5515834Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.568333Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 11
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 11, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create functions to merge turbine and weather data into their target Delta tables\n",
        "def merge_delta(incremental, target): \n",
        "  \n",
        "  incremental.dropDuplicates(['date','windowStart', 'windowEnd','deviceid']).createOrReplaceTempView(\"incremental\")\n",
        "  try:\n",
        "    # MERGE records into the target table using the specified join key\n",
        "    incremental._jdf.sparkSession().sql(f\"\"\"\n",
        "      MERGE INTO delta.`{target}` t\n",
        "      USING incremental i\n",
        "      ON i.date=t.date AND i.windowStart = t.windowStart AND i.windowEnd = t.windowEnd AND i.deviceId = t.deviceid\n",
        "      WHEN MATCHED THEN UPDATE SET *\n",
        "      WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        "  except:\n",
        "    # If the â€ arget table does not exist, create one\n",
        "    incremental.write.format(\"delta\").partitionBy(\"date\").save(target)\n",
        "    \n",
        "\n",
        "turbine_b_to_s = (\n",
        "  spark.readStream.format('delta').table(\"turbine_raw\")                        # Read data as a stream from our source Delta table\n",
        "    .groupBy('deviceId','date',F.window('timestamp','5 minutes'))              # Aggregate readings to hourly intervals\n",
        "    .agg(F.avg('rpm').alias('rpm'), F.avg(\"angle\").alias(\"angle\"))\n",
        "    .withColumn(\"windowStart\", F.col('window').start)\n",
        "    .withColumn(\"windowEnd\", F.col('window').end)\n",
        "    .drop(\"window\")\n",
        "    .writeStream                                                               # Write the resulting stream\n",
        "    .foreachBatch(lambda i, b: merge_delta(i, SILVER_PATH + \"turbine_agg\"))    # Pass each micro-batch to a function\n",
        "    .outputMode(\"update\")                                                      # Merge works with update mode\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_agg\")             # Checkpoint so we can restart streams gracefully\n",
        "    .start()\n",
        ")\n",
        "\n",
        "weather_b_to_s = (\n",
        "  spark.readStream.format('delta').table(\"weather_raw\")                        # Read data as a stream from our source Delta table\n",
        "    .groupBy('deviceid','date',F.window('timestamp','5 minutes'))              # Aggregate readings to hourly intervals\n",
        "    .agg({\"temperature\":\"avg\",\"humidity\":\"avg\",\"windspeed\":\"avg\",\"winddirection\":\"last\"})\n",
        "    .withColumn(\"windowStart\", F.col('window').start)\n",
        "    .withColumn(\"windowEnd\", F.col('window').end)\n",
        "    .selectExpr('date','windowStart','windowEnd','deviceid','`avg(temperature)` as temperature','`avg(humidity)` as humidity',\n",
        "                '`avg(windspeed)` as windspeed','`last(winddirection)` as winddirection')\n",
        "    .writeStream                                                               # Write the resulting stream\n",
        "    .foreachBatch(lambda i, b: merge_delta(i, SILVER_PATH + \"weather_agg\"))    # Pass each micro-batch to a function\n",
        "    .outputMode(\"update\")                                                      # Merge works with update mode\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"weather_agg\")             # Checkpoint so we can restart streams gracefully\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Create the external tables once data starts to stream in\n",
        "while True:\n",
        "  try:\n",
        "    spark.sql(f'CREATE TABLE IF NOT EXISTS turbine_agg USING DELTA LOCATION \"{SILVER_PATH + \"turbine_agg\"}\"')\n",
        "    spark.sql(f'CREATE TABLE IF NOT EXISTS weather_agg USING DELTA LOCATION \"{SILVER_PATH + \"weather_agg\"}\"')\n",
        "    break\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:05:30.6607712Z",
              "execution_start_time": "2021-10-12T03:03:49.4730779Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:00:22.570667Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 12
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 12, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read streams from Delta Silver tables and join them together on common columns (date & window)\n",
        "turbine_agg = spark.readStream.format('delta').option(\"ignoreChanges\", True).table('turbine_agg')\n",
        "weather_agg = spark.readStream.format('delta').option(\"ignoreChanges\", True).table('weather_agg').drop('deviceid')\n",
        "turbine_enriched = turbine_agg.join(weather_agg, ['date','windowStart', 'windowEnd'])\n",
        "\n",
        "# Write the stream to a foreachBatch function which performs the MERGE as before\n",
        "merge_gold_stream = (\n",
        "  turbine_enriched\n",
        "    .selectExpr('date','deviceid','windowStart','windowEnd','rpm','angle','temperature','humidity','windspeed','winddirection')\n",
        "    .writeStream \n",
        "    .foreachBatch(lambda i, b: merge_delta(i, GOLD_PATH + \"turbine_enriched\"))\n",
        "    .option(\"checkpointLocation\", CHECKPOINT_PATH + \"turbine_enriched\")         \n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Create the external tables once data starts to stream in\n",
        "while True:\n",
        "  try:\n",
        "    spark.sql(f'CREATE TABLE IF NOT EXISTS turbine_enriched USING DELTA LOCATION \"{GOLD_PATH + \"turbine_enriched\"}\"')\n",
        "    break\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-12T03:45:07.9194296Z",
              "execution_start_time": "2021-10-12T03:43:36.960352Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-12T03:43:36.4074443Z",
              "session_id": 14,
              "session_start_time": null,
              "spark_pool": "spark002v3",
              "state": "finished",
              "statement_id": 15
            },
            "text/plain": [
              "StatementMeta(spark002v3, 14, 15, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [
                [
                  "2021-10-12",
                  "WindTurbine-3",
                  "2021-10-12T03:20:00Z",
                  "2021-10-12T03:25:00Z",
                  8.567156334408242,
                  8.496261792607212,
                  28.868337378074187,
                  61.38425622937523,
                  6.138425622937524,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-2",
                  "2021-10-12T03:10:00Z",
                  "2021-10-12T03:15:00Z",
                  7.371602590170463,
                  6.450152266399155,
                  28.584211001690328,
                  69.44753502466052,
                  6.944753502466051,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-4",
                  "2021-10-12T03:30:00Z",
                  "2021-10-12T03:35:00Z",
                  7.129340388768368,
                  5.2381728401723215,
                  29.24704358023618,
                  62.67916096123534,
                  6.267916096123534,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-2",
                  "2021-10-12T03:30:00Z",
                  "2021-10-12T03:35:00Z",
                  9.124853608799325,
                  8.984246907699411,
                  28.390689543619597,
                  67.92157595837259,
                  6.79215759583726,
                  "NW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-6",
                  "2021-10-12T03:25:00Z",
                  "2021-10-12T03:30:00Z",
                  6.969132884493031,
                  7.0979912739314015,
                  23.459715258279807,
                  73.92220607834568,
                  7.3922206078345685,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-3",
                  "2021-10-12T03:30:00Z",
                  "2021-10-12T03:35:00Z",
                  6.870601788881848,
                  7.0117765652716155,
                  28.390689543619597,
                  67.92157595837259,
                  6.79215759583726,
                  "NW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-5",
                  "2021-10-12T03:30:00Z",
                  "2021-10-12T03:35:00Z",
                  7.868915404109132,
                  5.885300978595491,
                  29.24704358023618,
                  62.67916096123534,
                  6.267916096123534,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-1",
                  "2021-10-12T03:20:00Z",
                  "2021-10-12T03:25:00Z",
                  9.075175209904092,
                  7.940778308666081,
                  27.143314169955126,
                  67.98485403956894,
                  6.798485403956894,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-8",
                  "2021-10-12T03:25:00Z",
                  "2021-10-12T03:30:00Z",
                  8.03527075803196,
                  6.030861913277962,
                  23.459715258279807,
                  73.92220607834568,
                  7.3922206078345685,
                  "SW"
                ],
                [
                  "2021-10-12",
                  "WindTurbine-1",
                  "2021-10-12T03:30:00Z",
                  "2021-10-12T03:35:00Z",
                  7.112021962925416,
                  7.223019217559739,
                  28.390689543619597,
                  67.92157595837259,
                  6.79215759583726,
                  "NW"
                ]
              ],
              "schema": {
                "fields": [
                  {
                    "metadata": {},
                    "name": "date",
                    "nullable": true,
                    "type": "date"
                  },
                  {
                    "metadata": {},
                    "name": "deviceid",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "windowStart",
                    "nullable": true,
                    "type": "timestamp"
                  },
                  {
                    "metadata": {},
                    "name": "windowEnd",
                    "nullable": true,
                    "type": "timestamp"
                  },
                  {
                    "metadata": {},
                    "name": "rpm",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "angle",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "temperature",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "humidity",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "windspeed",
                    "nullable": true,
                    "type": "double"
                  },
                  {
                    "metadata": {},
                    "name": "winddirection",
                    "nullable": true,
                    "type": "string"
                  }
                ],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 10 rows and 10 fields>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql \n",
        "-- We can query the data directly from storage immediately as soon as it starts streams into Delta \n",
        "SELECT * FROM turbine_enriched LIMIT 10"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
